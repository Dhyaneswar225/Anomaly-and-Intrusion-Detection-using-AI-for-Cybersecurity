{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGK1rCA49ig0",
        "outputId": "e8175bec-3864-4386-b05d-674f6f385f0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/libpyg.so\n",
            "  import torch_geometric.typing\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_scatter/_version_cuda.so\n",
            "  import torch_geometric.typing\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_cluster/_version_cuda.so\n",
            "  import torch_geometric.typing\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_spline_conv/_version_cuda.so\n",
            "  import torch_geometric.typing\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_sparse/_version_cuda.so\n",
            "  import torch_geometric.typing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading processed data...\n",
            "Train samples: 125,973 | Test samples: 21,934\n",
            "Building graphs from normal training data...\n",
            "→ 13467 normal training graphs created\n",
            "Building test graphs...\n",
            "→ 4385 test graphs created\n",
            "Final test set: 100 normal + 600 attack windows\n",
            "Node feature dimension: 10\n",
            "Starting GNN training...\n",
            "  Epoch   1 | Loss: 0.013136\n",
            "  Epoch  20 | Loss: 0.000002\n",
            "  Epoch  40 | Loss: 0.000002\n",
            "  Epoch  60 | Loss: 0.000002\n",
            "  Epoch  80 | Loss: 0.000002\n",
            "Model saved → models/gnn_detector.pth\n",
            "Evaluating on test graphs...\n",
            "Threshold (95th percentile on normal): 0.000001\n",
            "\n",
            "============================================================\n",
            "GNN Anomaly Detector Results\n",
            "============================================================\n",
            "  Graphs evaluated  : 700\n",
            "  ROC-AUC           : 0.839233\n",
            "  PR-AUC            : 0.968961\n",
            "  Accuracy          : 0.765714\n",
            "  F1 Score          : 0.849541\n",
            "  Precision@10%     : 1.000000\n",
            "  Threshold         : 0.000001\n",
            "============================================================\n",
            "\n",
            "All results saved in 'results'\n",
            "GNN training and evaluation completed successfully!\n"
          ]
        }
      ],
      "source": [
        "# src/gnn_model.py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.data import Data\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, precision_recall_curve, auc,\n",
        "    accuracy_score, f1_score\n",
        ")\n",
        "\n",
        "# ============================= CONFIG =============================\n",
        "DATA_PROCESSED = \"/content\"  # Local path (not Colab)\n",
        "RESULTS_DIR = \"results\"\n",
        "MODELS_DIR = \"models\"\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "WINDOW_SIZE = 10\n",
        "STRIDE = 5\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 80\n",
        "MAX_ATTACK_RATIO = 0.30  # For test set balancing\n",
        "\n",
        "# ============================= GRAPH BUILDER =============================\n",
        "def build_graphs_with_ratio(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Builds sliding window graphs.\n",
        "    Returns list of (graph, attack_ratio_in_window)\n",
        "    \"\"\"\n",
        "    graphs = []\n",
        "    label_col = \"label_binary\"  # Use the new binary label\n",
        "\n",
        "    for start in range(0, len(df) - WINDOW_SIZE + 1, STRIDE):\n",
        "        win = df.iloc[start:start + WINDOW_SIZE].copy()\n",
        "        if len(win) < 8:\n",
        "            continue\n",
        "\n",
        "        # Create node identifiers using 'service' (or fallback to index if missing)\n",
        "        if 'service' in win.columns:\n",
        "            win['src_node'] = win['service'].astype(str) + '_src'\n",
        "            win['dst_node'] = win['service'].astype(str) + '_dst'\n",
        "        else:\n",
        "            # Fallback: use row index as unique identifier\n",
        "            win['src_node'] = win.index.astype(str) + '_src'\n",
        "            win['dst_node'] = win.index.astype(str) + '_dst'\n",
        "\n",
        "        nodes = pd.unique(win[['src_node', 'dst_node']].values.ravel('K'))\n",
        "        if len(nodes) < 2:\n",
        "            continue\n",
        "\n",
        "        node2idx = {node: idx for idx, node in enumerate(nodes)}\n",
        "\n",
        "        edge_index = []\n",
        "        edge_attr = []\n",
        "        feature_cols = ['duration', 'src_bytes', 'dst_bytes', 'count',\n",
        "                        'same_srv_rate', 'diff_srv_rate', 'serror_rate', 'rerror_rate']\n",
        "\n",
        "        for _, row in win.iterrows():\n",
        "            src = node2idx[row['src_node']]\n",
        "            dst = node2idx[row['dst_node']]\n",
        "            edge_index.append([src, dst])\n",
        "            attr = []\n",
        "            for col in feature_cols:\n",
        "                val = row[col] if col in row else 0.0\n",
        "                attr.append(float(val))\n",
        "            edge_attr.append(attr)\n",
        "\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
        "\n",
        "        # Node features: aggregated stats per node\n",
        "        node_feat = []\n",
        "        for node in nodes:\n",
        "            mask = (win['src_node'] == node) | (win['dst_node'] == node)\n",
        "            node_data = win[mask]\n",
        "            if len(node_data) == 0:\n",
        "                feats = [0.0] * len(feature_cols)\n",
        "            else:\n",
        "                feats = [\n",
        "                    node_data['duration'].mean(),\n",
        "                    node_data['src_bytes'].mean(),\n",
        "                    node_data['dst_bytes'].mean(),\n",
        "                    node_data['count'].mean(),\n",
        "                    node_data['same_srv_rate'].mean(),\n",
        "                    node_data['diff_srv_rate'].mean(),\n",
        "                    node_data['serror_rate'].mean(),\n",
        "                    node_data['rerror_rate'].mean(),\n",
        "                ]\n",
        "            feats += [len(node_data), 1.0 if 'http' in str(node) else 0.0]\n",
        "            node_feat.append(feats)\n",
        "\n",
        "        x = torch.tensor(node_feat, dtype=torch.float)\n",
        "\n",
        "        # Label: 1 if majority of connections in window are attacks\n",
        "        attack_ratio = (win[label_col] == \"attack\").mean()\n",
        "        y = 1 if attack_ratio > 0.5 else 0\n",
        "\n",
        "        graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=torch.tensor([y], dtype=torch.float))\n",
        "        graphs.append((graph, attack_ratio))\n",
        "\n",
        "    return graphs\n",
        "\n",
        "\n",
        "# ============================= MODEL =============================\n",
        "class GNNAnomalyDetector(nn.Module):\n",
        "    def __init__(self, input_dim, hidden=128, layers=3):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList(\n",
        "            [GCNConv(input_dim, hidden)] +\n",
        "            [GCNConv(hidden, hidden) for _ in range(layers - 1)]\n",
        "        )\n",
        "        self.lin1 = nn.Linear(hidden, hidden // 2)\n",
        "        self.lin2 = nn.Linear(hidden // 2, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.drop = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = self.relu(x)\n",
        "            x = self.drop(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.relu(self.lin1(x))\n",
        "        x = self.drop(x)\n",
        "        return self.lin2(x).squeeze(-1)\n",
        "\n",
        "\n",
        "# ============================= TRAINING & EVALUATION =============================\n",
        "def train_gnn():\n",
        "    print(\"Loading processed data...\")\n",
        "    train_df = pd.read_csv(f\"{DATA_PROCESSED}/train_processed.csv\")\n",
        "    test_df  = pd.read_csv(f\"{DATA_PROCESSED}/test_processed.csv\")\n",
        "\n",
        "    if \"label_binary\" not in train_df.columns:\n",
        "        raise ValueError(\"Column 'label_binary' not found! Required for GNN.\")\n",
        "\n",
        "    print(f\"Train samples: {len(train_df):,} | Test samples: {len(test_df):,}\")\n",
        "\n",
        "    # ============================= BUILD GRAPHS =============================\n",
        "    print(\"Building graphs from normal training data...\")\n",
        "    normal_train = train_df[train_df[\"label_binary\"] == \"normal\"].copy()\n",
        "    train_graphs_with_ratio = build_graphs_with_ratio(normal_train)\n",
        "    train_graphs = [g for g, _ in train_graphs_with_ratio]\n",
        "    print(f\"→ {len(train_graphs)} normal training graphs created\")\n",
        "\n",
        "    print(\"Building test graphs...\")\n",
        "    test_graphs_with_ratio = build_graphs_with_ratio(test_df)\n",
        "    test_graphs = [g for g, _ in test_graphs_with_ratio]\n",
        "    print(f\"→ {len(test_graphs)} test graphs created\")\n",
        "\n",
        "    # Balanced test set: some clean windows + attack windows\n",
        "    clean_windows = [g for g, r in test_graphs_with_ratio if r <= MAX_ATTACK_RATIO]\n",
        "    attack_windows = [g for g in test_graphs if g.y.item() == 1]\n",
        "\n",
        "    # Limit for faster evaluation (remove in production)\n",
        "    clean_windows = clean_windows[:100]\n",
        "    attack_windows = attack_windows[:600]\n",
        "\n",
        "    final_test_graphs = clean_windows + attack_windows\n",
        "    print(f\"Final test set: {len(clean_windows)} normal + {len(attack_windows)} attack windows\")\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_graphs, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader  = DataLoader(final_test_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    if len(train_graphs) == 0:\n",
        "        raise ValueError(\"No training graphs generated! Check data and features.\")\n",
        "\n",
        "    input_dim = train_graphs[0].x.shape[1]\n",
        "    print(f\"Node feature dimension: {input_dim}\")\n",
        "\n",
        "    model = GNNAnomalyDetector(input_dim=input_dim, hidden=128, layers=3).to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # ============================= TRAINING =============================\n",
        "    print(\"Starting GNN training...\")\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for data in train_loader:\n",
        "            data = data.to(DEVICE)\n",
        "            data.y = torch.zeros(data.num_graphs, dtype=torch.float, device=DEVICE)  # All normal\n",
        "            logits = model(data)\n",
        "            loss = criterion(logits, data.y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        if epoch % 20 == 0 or epoch == 1:\n",
        "            avg_loss = epoch_loss / len(train_loader)\n",
        "            print(f\"  Epoch {epoch:3d} | Loss: {avg_loss:.6f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), f\"{MODELS_DIR}/gnn_detector.pth\")\n",
        "    print(f\"Model saved → {MODELS_DIR}/gnn_detector.pth\")\n",
        "\n",
        "    # ============================= EVALUATION =============================\n",
        "    print(\"Evaluating on test graphs...\")\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Score on normal training graphs (for threshold)\n",
        "        train_scores = []\n",
        "        for data in train_loader:\n",
        "            data = data.to(DEVICE)\n",
        "            logits = model(data)\n",
        "            train_scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "        threshold = np.percentile(train_scores, 95)\n",
        "        print(f\"Threshold (95th percentile on normal): {threshold:.6f}\")\n",
        "\n",
        "        # Score on test set\n",
        "        for data in test_loader:\n",
        "            data = data.to(DEVICE)\n",
        "            logits = model(data)\n",
        "            scores.extend(torch.sigmoid(logits).cpu().numpy())\n",
        "            true_labels.extend(data.y.cpu().numpy())\n",
        "\n",
        "    scores = np.array(scores)\n",
        "    true_labels = np.array(true_labels).astype(int)\n",
        "\n",
        "    y_pred = (scores > threshold).astype(int)\n",
        "\n",
        "    # Metrics\n",
        "    auc_roc = roc_auc_score(true_labels, scores)\n",
        "    prec, rec, _ = precision_recall_curve(true_labels, scores)\n",
        "    auc_pr = auc(rec, prec)\n",
        "    accuracy = accuracy_score(true_labels, y_pred)\n",
        "    f1 = f1_score(true_labels, y_pred)\n",
        "    k = max(1, int(0.1 * len(scores)))\n",
        "    precision_at_10 = np.mean(true_labels[np.argsort(scores)[-k:]])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"GNN Anomaly Detector Results\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"  Graphs evaluated  : {len(scores)}\")\n",
        "    print(f\"  ROC-AUC           : {auc_roc:.6f}\")\n",
        "    print(f\"  PR-AUC            : {auc_pr:.6f}\")\n",
        "    print(f\"  Accuracy          : {accuracy:.6f}\")\n",
        "    print(f\"  F1 Score          : {f1:.6f}\")\n",
        "    print(f\"  Precision@10%     : {precision_at_10:.6f}\")\n",
        "    print(f\"  Threshold         : {threshold:.6f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Save results\n",
        "    pd.DataFrame({\n",
        "        \"anomaly_score\": scores,\n",
        "        \"true_label\": true_labels,\n",
        "        \"predicted_label\": y_pred\n",
        "    }).to_csv(f\"{RESULTS_DIR}/gnn_scores.csv\", index=False)\n",
        "\n",
        "    # Plot score distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(scores[true_labels == 0], label=\"Normal Windows\", alpha=0.7, bins=40, color=\"blue\")\n",
        "    sns.histplot(scores[true_labels == 1], label=\"Attack Windows\", alpha=0.7, bins=40, color=\"red\")\n",
        "    plt.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f\"Threshold = {threshold:.4f}\")\n",
        "    plt.title(\"GNN Anomaly Score Distribution (Test Set)\")\n",
        "    plt.xlabel(\"Anomaly Score\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{RESULTS_DIR}/gnn_score_distribution.png\", dpi=200, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"\\nAll results saved in '{RESULTS_DIR}'\")\n",
        "    print(\"GNN training and evaluation completed successfully!\")\n",
        "\n",
        "\n",
        "# ============================= RUN =============================\n",
        "if __name__ == \"__main__\":\n",
        "    train_gnn()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}